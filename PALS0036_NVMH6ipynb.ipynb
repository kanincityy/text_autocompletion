{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PALS0039 Introduction to Deep Learning for Speech and Language Processing  \n",
        "\n",
        "###**Asessment Coursework: Autocompletion Task**\n",
        "\n",
        "####Student ID: NVMH6\n",
        "\n",
        "####Word Count:"
      ],
      "metadata": {
        "id": "jrj8opz8NK2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "\n",
        "As highlighted in the task description, humans can complete words, sentences, and sounds even when parts are missing or masked by noise. Text-editing software emulates this by providing text suggestions. This coursework involves building such an autocompletion system, a technology that enhances efficiency and is prevalent in everyday text-based communication like texting and document writing."
      ],
      "metadata": {
        "id": "LDmdM4tERg_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Wrangling & Environment Set up\n",
        "\n",
        "The data pre-processing section is split into the following code blocks:\n",
        "\n",
        "1. Load relevant libraries and functions\n",
        "2. Create a function to load the text files from the urls available on Moodle\n",
        "3. Create a TextCharacterDataset class with the parent Dataset class from Pytorch\n",
        "4. Load and inspect the data"
      ],
      "metadata": {
        "id": "dVP4SKHWMr5X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LY207a8BMpSA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import requests\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load url function\n",
        "\n",
        "This is just a basic function that will ensure the urls are loaded properly. I have decided to keep it apart from the Dataset class as I don't want to overload the class with another function."
      ],
      "metadata": {
        "id": "vUNpgBWAgVUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function that will load the text file from the provided url\n",
        "def load_text_from_url(url):\n",
        "    \"\"\"\n",
        "    Loads text content from a given URL.\n",
        "\n",
        "    Args:\n",
        "        url: The URL of the text file.\n",
        "\n",
        "    Returns:\n",
        "        The text content as a string, or None if the download fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading text from {url}: {e}\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "NottQ6Q2T4uK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the TextCharacterDataset class\n",
        "\n",
        "I have decided to implement a Dataset class from Pytorch which makes the code efficient and organised at pre-processing the data needed to train my model. To train an RNN, I must turn all the available text files into contexts and tokens to predict.  I will be training a character-based model, meaning a model that predicts the next three characters based on all the previously given characters.\n",
        "\n",
        "In order to do this, it is a good idea to select a context window, by keeping the context window consistent across my training data, I will be able to train my model much faster. Which is why, I will truncate each example to a set length, and add a special \\<PAD> token to the beginning of the context if it is shorter than the stated context length.\n",
        "\n",
        "This also means that I can build up both my char2index dictionary and characters list at the same time."
      ],
      "metadata": {
        "id": "qNfjL3hRcasu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class that can be initiated with relevant text files\n",
        "class TextCharacterDataset(Dataset):\n",
        "\n",
        "    def __init__(self, text_urls, context_size):\n",
        "      # store the path to the text files\n",
        "        self.text_urls = text_urls\n",
        "        # set context size\n",
        "        self.context_size = context_size\n",
        "        # call load_data to populate self.char2index and self.characters\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Loads and processes text from the specified URLs.\n",
        "        \"\"\"\n",
        "        self.char2index = {\"<PAD>\": 0}\n",
        "        self.characters = []\n",
        "        next_index = 1\n",
        "        # Iterate through each url\n",
        "        for url in self.text_urls:\n",
        "          #\n",
        "            text = load_text_from_url(url)  # Get text from URL\n",
        "            if text:  # Only process if text was loaded successfully\n",
        "                for char in text:\n",
        "                    if char not in self.char2index:\n",
        "                        self.char2index[char] = next_index\n",
        "                        next_index += 1\n",
        "                # Extend the characters list\n",
        "                self.characters.extend([self.char2index[char] for char in text])\n",
        "            else:\n",
        "                print(f\"Skipping URL: {url} due to download error.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.characters) - self.context_size - 3 + 1\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        start_idx = max(0, index)\n",
        "        end_idx = start_idx + self.context_size\n",
        "        context = self.characters[start_idx:end_idx]\n",
        "\n",
        "        target_start_idx = end_idx\n",
        "        target_end_idx = target_start_idx + 3\n",
        "        targets = self.characters[target_start_idx:target_end_idx]\n",
        "\n",
        "        # Pad context if necessary\n",
        "        if len(context) < self.context_size:\n",
        "            context = [self.char2index[\"<PAD>\"]] * (self.context_size - len(context)) + context\n",
        "\n",
        "        # Pad targets if necessary\n",
        "        if len(targets) < 3:\n",
        "            targets = targets + [self.char2index[\"<PAD>\"]] * (3 - len(targets))\n",
        "\n",
        "        input_ids = torch.LongTensor(context)\n",
        "        target_ids = torch.LongTensor(targets)\n",
        "\n",
        "        return input_ids, target_ids"
      ],
      "metadata": {
        "id": "q7KutYtPcYlh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of the 7 books available on Project Gutenberg\n",
        "text_urls = [\n",
        "    \"https://www.gutenberg.org/cache/epub/345/pg345.txt\",\n",
        "    \"https://www.gutenberg.org/cache/epub/84/pg84.txt\",\n",
        "    \"https://www.gutenberg.org/cache/epub/74/pg74.txt\",\n",
        "    \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\",\n",
        "    \"https://www.gutenberg.org/cache/epub/1727/pg1727.txt\",\n",
        "    \"https://www.gutenberg.org/cache/epub/2701/pg2701.txt\",\n",
        "    \"https://www.gutenberg.org/cache/epub/3207/pg3207.txt\"\n",
        "]\n",
        "\n",
        "# Set a context size window\n",
        "context_size = 50\n",
        "# Creating an instance of Dataset class\n",
        "dataset = TextCharacterDataset(text_urls, context_size)\n",
        "\n",
        "# Checking length of Dataset\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "\n",
        "# Inspecting content of sentence[10]\n",
        "input_ids, target_ids = dataset[10]\n",
        "print(\"Input:\", input_ids)\n",
        "print(\"Target:\", target_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0ntwPJbWj3O",
        "outputId": "c7ae8b5b-0efe-4b56-e3ab-0f90f031b1ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 5737424\n",
            "Input: tensor([10, 11,  5, 12, 13, 11,  4, 14, 15,  4,  7, 16,  5,  4, 17,  8,  8, 18,\n",
            "         5,  8, 19,  5, 20,  7, 21, 10, 13, 22, 21, 23, 24,  5,  5,  5,  5, 23,\n",
            "        24,  2,  3, 25, 26,  5,  4, 15,  8,  8, 18,  5, 25, 26])\n",
            "Target: tensor([ 5, 19,  8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Implementation\n",
        "\n",
        "I have selected an LSTM model from the RNN family because it is a model that is capable of capturing context and long-term dependencies. I believe that this will be highly beneficial to the autocompletion task.\n",
        "\n",
        "This section is broken down into the following code blocks:\n",
        "1. Defining the LSTM network class\n",
        "2. Training the model\n",
        "3. Model evaluation\n",
        "3. Creating a user-friendly interface for the model"
      ],
      "metadata": {
        "id": "s6jRU3RMauWu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "naDD7EPwbT0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limitations & Conclusion"
      ],
      "metadata": {
        "id": "iiRscdzZbUdr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsSRHoITguE_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}